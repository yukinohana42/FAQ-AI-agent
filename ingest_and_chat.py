# ingest_and_chat.py  â”€â”€ â€œå›žç­”â€ ã ã‘ã‚’è¿”ã™ç°¡æ½”ç‰ˆï¼ˆLangChain/LangGraphæœ€æ–°ç³»å¯¾å¿œï¼‰
from dotenv import load_dotenv
load_dotenv()

import os, glob, re, textwrap
from collections import Counter

try:
    from termcolor import colored
except ModuleNotFoundError:
    def colored(t, *_a, **_k): return t

# æœ€æ–°: RecursiveCharacterTextSplitter ã¯å°‚ç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«åˆ†é›¢
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import (
    GoogleGenerativeAIEmbeddings,
    ChatGoogleGenerativeAI,
)

# è¨­å®š
INDEX_PATH   = "faiss_index"
DOCS_DIR     = "docs"
CHUNK_SIZE   = 500
CHUNK_OVERLP = 350
NEARBY_RANGE = 2

EMBED_MODEL  = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
LLM          = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0)
DEBUG = False

MED_SYNONYMS = {
    "ãƒŽãƒ«ã‚¢ãƒ‰ãƒ¬ãƒŠãƒªãƒ³": ["ãƒŽãƒ«ã‚¢ãƒ‰ãƒ¬ãƒŠãƒªãƒ³", "ãƒŽãƒ«ã‚¢ãƒ‰", "NAd", "noradrenaline", "NA"],
}

def auto_expand_keywords(keywords: list[str]) -> list[str]:
    joined = ", ".join(keywords)
    prompt = (
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {joined}\n"
        "åŒç¾©èªžãƒ»åŒ»è–¬å“åãƒ»ç•¥èªžãƒ»é–¢é€£èªžã‚’æ—¥æœ¬èªž/è‹±èªžã§æœ€å¤§10å€‹ã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šã§åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚"
    )
    resp = LLM.invoke(prompt)
    return list({kw.strip() for kw in (keywords + resp.content.split(",")) if kw.strip()})

def extract_main_keywords(query: str) -> list[str]:
    tokens = re.findall(r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³a-zA-Z0-9\-ï¼‹ãƒ».]+", query)
    kws: list[str] = []
    for w in tokens:
        hit = next((syns for syns in MED_SYNONYMS.values() if w in syns), None)
        kws.extend(hit if hit else [w])
    kws = list(set(kws))
    expanded = auto_expand_keywords(kws)
    if DEBUG:
        print(colored(f"ðŸ”‘ å±•é–‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {expanded}", "yellow"))
    return expanded

def build_index() -> None:
    docx_files = glob.glob(os.path.join(DOCS_DIR, "*.docx"))
    if not docx_files:
        raise FileNotFoundError("./docs ã« .docx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    raw_docs = []
    for p in docx_files:
        for i, d in enumerate(UnstructuredWordDocumentLoader(p).load()):
            d.metadata.update(source=os.path.basename(p), chunk_id=i)
            raw_docs.append(d)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLP
    )
    chunks = splitter.split_documents(raw_docs)
    if DEBUG:
        print(colored(f"ðŸ”¹ ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}", "cyan"))

    vecdb = None
    batch: list = []
    for idx, doc in enumerate(chunks, 1):
        batch.append(doc)
        if len(batch) == 100 or idx == len(chunks):
            vecdb = FAISS.from_documents(batch, EMBED_MODEL) if vecdb is None else vecdb.add_documents(batch)
            batch = []
    vecdb.save_local(INDEX_PATH)
    if DEBUG:
        print(colored(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜ ({INDEX_PATH})", "green"))

def embedding_search(vecdb, query, k=30):
    return vecdb.similarity_search(query, k=k)

def keyword_score(doc, keywords):
    return sum(len(re.findall(re.escape(kw), doc.page_content, re.I)) for kw in keywords)

def keyword_search_multi(vecdb, keywords, k=30):
    docs = vecdb.docstore._dict.values()
    hits = [
        d for d in docs if any(re.search(re.escape(kw), d.page_content, re.I) for kw in keywords)
    ]
    hits.sort(key=lambda d: keyword_score(d, keywords), reverse=True)
    return hits[:k]

def filter_by_keywords(docs, keywords):
    if len(keywords) >= 2:
        all_hit = [d for d in docs if all(kw.lower() in d.page_content.lower() for kw in keywords)]
        if all_hit:
            return all_hit
    return [d for d in docs if any(kw.lower() in d.page_content.lower() for kw in keywords)]

def add_nearby_chunks(vecdb, selected):
    results = list(selected)
    done = {(d.metadata["source"], d.metadata["chunk_id"]) for d in selected}

    for d in selected:
        sid, cid = d.metadata["source"], d.metadata["chunk_id"]
        for nid in range(cid - NEARBY_RANGE, cid + NEARBY_RANGE + 1):
            if nid < 0 or (sid, nid) in done:
                continue
            maybe = next(
                (
                    cand
                    for cand in vecdb.docstore._dict.values()
                    if cand.metadata.get("source") == sid and cand.metadata.get("chunk_id") == nid
                ),
                None,
            )
            if maybe:
                results.append(maybe)
                done.add((sid, nid))
    return results

def make_summary(results, query, keywords):
    uniq_text = "\n---\n".join({d.page_content for d in results})
    prompt = f"""
ã‚ãªãŸã¯æ•‘æ€¥ãƒ»ç·åˆå†…ç§‘ã®ãƒ™ãƒ†ãƒ©ãƒ³åŒ»å¸«ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: ã€Œ{query}ã€

## åŸ·ç­†æŒ‡ç¤º
1. ä»¥ä¸‹ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æŠ½å‡ºã—ã€è‡¨åºŠçš„æŽ¨è«–ã§è£œå®Œã—ã¦ **æ–­å®šçš„** ã«ã¾ã¨ã‚ã‚‹ã€‚
2. è–¬å‰¤åãƒ»ç”¨é‡ãƒ»æŠ•ä¸ŽçµŒè·¯ãƒ»ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãªã©å¿…é ˆãƒã‚¤ãƒ³ãƒˆã¯ **æ˜Žç¢ºã«åˆ—æŒ™**ã€‚
3. æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¦ã‚‚ã€æ¨™æº–æ²»ç™‚ã‚’æ ¹æ‹ ã«â€åŒ»å¸«ã¨ã—ã¦ã®æœ€é©è§£â€ã‚’ç¤ºã™ã€‚
4. ä¸è¦ãªå‰ç½®ãã‚„ä¸€èˆ¬è«–ã¯æ›¸ã‹ãªã„ã€‚

--- å‚è€ƒ ---
{uniq_text}
--- ã“ã“ã¾ã§ ---
"""
    return LLM.invoke(prompt).content.strip()

def agent_search_and_pick(vecdb, query, max_retry=3, k=40):
    keywords = extract_main_keywords(query)

    for _ in range(max_retry):
        emb = embedding_search(vecdb, query, k=k)
        cand = add_nearby_chunks(vecdb, filter_by_keywords(emb, keywords))
        if cand:
            return cand, keywords
        query = LLM.invoke(f"ã€Œ{query}ã€ã‚’æ—¥æœ¬èªžã§åˆ¥è¡¨ç¾ 1 ã¤").content.strip()

    kw_hits = add_nearby_chunks(vecdb, keyword_search_multi(vecdb, keywords, k=k))
    return kw_hits, keywords
