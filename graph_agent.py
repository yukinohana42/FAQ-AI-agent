"""
graph_agent.py â€“ LangGraph + ingest_and_chat å®‰å®šå®Ÿè£… (ãƒ¬ãƒ™ãƒ«2ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«å¼·åŒ–ç‰ˆ + è³ªå•ãƒ­ã‚°è¨˜éŒ²)
"""
from __future__ import annotations
import os
import re
from typing import TypedDict, List, Optional
from langgraph.graph import StateGraph
from ingest_and_chat import build_index, agent_search_and_pick, make_summary
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from eval_utils import log_user_question  # è³ªå•ãƒ­ã‚°è¨˜éŒ²ç”¨ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

# â”€â”€ ã‚°ãƒ©ãƒ•å…¨ä½“ã§å…±æœ‰ã™ã‚‹çŠ¶æ…‹ã®å‹ã‚’å®šç¾© ---------------------
class GraphState(TypedDict):
    query: str
    hits: Optional[List[Document]]
    keywords: Optional[List[str]]
    answer: Optional[str]

# â”€â”€ Model -------------------------------------------------
EMBED = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
LLM   = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0)

# â”€â”€ Vector DB ---------------------------------------------
VECDB: FAISS = FAISS.load_local(
    "faiss_index",
    EMBED,
    allow_dangerous_deserialization=True,
)

# â”€â”€ ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•° --------------------------------------
def contains_pii(text: str) -> bool:
    patterns = [
        r'\b[\w.-]+@[\w.-]+\.\w+\b',        # ãƒ¡ãƒ¼ãƒ«
        r'\b\d{2,4}-\d{2,4}-\d{4}\b',      # é›»è©±ç•ªå·
        r'\b\d{3}-\d{4}\b',                # éƒµä¾¿ç•ªå·
    ]
    return any(re.search(pat, text) for pat in patterns)

def contains_inappropriate(text: str) -> bool:
    ng_words = ["å·®åˆ¥çš„è¡¨ç¾ä¾‹", "ä¾®è¾±çš„è¡¨ç¾ä¾‹", "ä¸é©åˆ‡ãƒ¯ãƒ¼ãƒ‰ä¾‹"]  # å¿…è¦ã«å¿œã˜ã¦æ‹¡å……
    return any(word in text for word in ng_words)

def llm_output_safety_check(text: str) -> bool:
    prompt = (
        f"æ¬¡ã®å‡ºåŠ›ã«å€‹äººæƒ…å ±ã€å·®åˆ¥è¡¨ç¾ã€èª¤æƒ…å ±ã€ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆã€ä¸é©åˆ‡è¡¨ç¾ã€ã¾ãŸã¯ã‚·ã‚¹ãƒ†ãƒ ã®ç§˜å¯†æƒ…å ±ã‚’æ¼æ´©ã™ã‚‹å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ "
        f"å‡ºåŠ›:ã€Œ{text}ã€ å«ã¾ã‚Œã‚‹å ´åˆã¯Yesã€å«ã¾ã‚Œãªã„å ´åˆã¯Noã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚"
    )
    resp = LLM.invoke(prompt)
    decision = resp.content.strip().lower()
    print(f"[LLM Output Safety Check] åˆ¤å®šçµæœ: {decision}")
    return decision.startswith("no")

def llm_input_safety_check(text: str) -> bool:
    prompt = (
        f"æ¬¡ã®è³ªå•ã¯ã€å€‹äººæƒ…å ±ã‚’åé›†ã—ã‚ˆã†ã¨ã—ãŸã‚Šã€å·®åˆ¥ã€æš´åŠ›ã€é•æ³•è¡Œç‚ºã€ã‚·ã‚¹ãƒ†ãƒ ä¾µå®³ã‚’è©¦ã¿ã‚‹ã‚ˆã†ãªå†…å®¹ã§ã™ã‹ï¼Ÿ "
        f"è³ªå•:ã€Œ{text}ã€ å«ã¾ã‚Œã‚‹å ´åˆã¯Yesã€å«ã¾ã‚Œãªã„å ´åˆã¯Noã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚"
    )
    resp = LLM.invoke(prompt)
    decision = resp.content.strip().lower()
    print(f"[LLM Input Safety Check] åˆ¤å®šçµæœ: {decision}")
    return decision.startswith("no")

# â”€â”€ Node & Router Function ----------------------------------
def router(state: GraphState) -> str:
    q = state["query"]

    if not llm_input_safety_check(q):
        print("âš ï¸ å…¥åŠ›ã®å®‰å…¨æ€§ã«æ‡¸å¿µ: å‡ºåŠ›ãƒ–ãƒ­ãƒƒã‚¯")
        state["answer"] = "è³ªå•å†…å®¹ã«å®‰å…¨æ€§ã®æ‡¸å¿µãŒã‚ã‚‹ãŸã‚ã€å›ç­”ã‚’æ§ãˆã¾ã™ã€‚"
        return "summarize"

    need = LLM.invoke(
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ãŒå¿…è¦ã§ã™ã‹ï¼Ÿ è³ªå•ï¼š ã€Œ{q}ã€\næ¤œç´¢ãŒå¿…è¦ãªå ´åˆã¯ 'Yes'ã€ä¸è¦ãªå ´åˆã¯ 'No' ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚"
    ).content.strip().lower()

    decision = "rag" if need.startswith('y') else "direct_answer"
    print(f"  [Router] åˆ¤æ–­: {decision}")
    return decision

def rag(state: GraphState) -> dict:
    print("  [Node] RAGæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
    q = state["query"]
    hits, kw = agent_search_and_pick(VECDB, q)
    return {"hits": hits, "keywords": kw}

def summarize(state: GraphState) -> dict:
    q = state["query"]
    hits = state.get("hits")
    keywords = state.get("keywords")

    if state.get("answer"):
        return {"answer": state["answer"]}

    if hits:
        print("  [Node] æ¤œç´¢çµæœã‚’å…ƒã«å›ç­”ã‚’ç”Ÿæˆä¸­...")
        answer = make_summary(hits, q, keywords)
    else:
        print("  [Node] æ¤œç´¢çµæœãŒãªã„ãŸã‚ã€ç›´æ¥å›ç­”ã‚’ç”Ÿæˆä¸­...")
        answer = LLM.invoke(f"200å­—ä»¥å†…ã§ã€åŒ»å¸«ã¨ã—ã¦å°‚é–€çš„ã‹ã¤æ–­å®šçš„ã«ç­”ãˆã¦ä¸‹ã•ã„: {q}").content

    if contains_pii(answer):
        print("âš ï¸ PIIæ¤œå‡º: å‡ºåŠ›ãƒ–ãƒ­ãƒƒã‚¯")
        return {"answer": "å‡ºåŠ›ã«å€‹äººæƒ…å ±ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å›ç­”ã‚’æ§ãˆã¾ã™ã€‚"}
    
    if contains_inappropriate(answer):
        print("âš ï¸ ä¸é©åˆ‡è¡¨ç¾æ¤œå‡º: å‡ºåŠ›ãƒ–ãƒ­ãƒƒã‚¯")
        return {"answer": "å‡ºåŠ›å†…å®¹ã«ä¸é©åˆ‡ãªè¡¨ç¾ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å›ç­”ã‚’æ§ãˆã¾ã™ã€‚"}
    
    if not llm_output_safety_check(answer):
        print("âš ï¸ LLMå‡ºåŠ›å®‰å…¨åˆ¤å®šNG: å‡ºåŠ›ãƒ–ãƒ­ãƒƒã‚¯")
        return {"answer": "å‡ºåŠ›ã®å®‰å…¨æ€§ã«æ‡¸å¿µãŒã‚ã‚‹ãŸã‚ã€å›ç­”ã‚’æ§ãˆã¾ã™ã€‚"}

    return {"answer": answer}

# â”€â”€ Graph -------------------------------------------------
graph = StateGraph(GraphState)
graph.add_node("rag", rag)
graph.add_node("summarize", summarize)

graph.set_conditional_entry_point(
    router,
    {
        "rag": "rag",
        "direct_answer": "summarize",
    }
)

graph.add_edge("rag", "summarize")
chat_agent = graph.compile()

# â”€â”€ CLI ----------------------------------------------------
if __name__ == "__main__":
    print("AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒèµ·å‹•ã—ã¾ã—ãŸã€‚è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(çµ‚äº†ã™ã‚‹ã«ã¯ exit ã¾ãŸã¯ quit)")
    try:
        while True:
            q = input("\nğŸ—¨ï¸  > ").strip()
            if q.lower() in {"exit", "quit"}:
                print("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            if not q:
                continue

            log_user_question(q)  # è³ªå•ãƒ­ã‚°è¨˜éŒ²

            res = chat_agent.invoke({"query": q})

            print("\nâœ… å›ç­”\n" + "=" * 20)
            print(res["answer"].strip())
            print("=" * 20 + "\n")

    except (EOFError, KeyboardInterrupt):
        print("\nã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
