"""
graph_agent.py â€“ LangGraph + ingest_and_chat å®‰å®šå®Ÿè£… (æœ€çµ‚å®Œæˆç‰ˆ)
"""
from __future__ import annotations
import os
from typing import TypedDict, List, Optional
from langgraph.graph import StateGraph
from ingest_and_chat import build_index, agent_search_and_pick, make_summary
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# â”€â”€ ã‚°ãƒ©ãƒ•å…¨ä½“ã§å…±æœ‰ã™ã‚‹çŠ¶æ…‹ã®å‹ã‚’å®šç¾© ---------------------
class GraphState(TypedDict):
    query: str
    hits: Optional[List[Document]]
    keywords: Optional[List[str]]
    answer: Optional[str]

# â”€â”€ Model -------------------------------------------------
EMBED = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
LLM   = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0)

# â”€â”€ Vector DB ---------------------------------------------
if not os.path.exists("faiss_index"):
    print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ–°è¦ã«ä½œæˆã—ã¾ã™...")
    build_index()
    print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

VECDB: FAISS = FAISS.load_local(
    "faiss_index",
    EMBED,
    allow_dangerous_deserialization=True,
)

# â”€â”€ Node & Router Function ----------------------------------
def router(state: GraphState) -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•å†…å®¹ã«å¿œã˜ã¦ã€RAGæ¤œç´¢ã‚’è¡Œã†ã‹ã€ç›´æ¥LLMã«å›ç­”ã•ã›ã‚‹ã‹ã‚’åˆ¤æ–­ã™ã‚‹"""
    q = state["query"]
    need = LLM.invoke(
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ãŒå¿…è¦ã§ã™ã‹ï¼Ÿ è³ªå•ï¼š ã€Œ{q}ã€\næ¤œç´¢ãŒå¿…è¦ãªå ´åˆã¯ 'Yes'ã€ä¸è¦ãªå ´åˆã¯ 'No' ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚"
    ).content.strip().lower()
    
    decision = "rag" if need.startswith('y') else "direct_answer"
    print(f"  [Router] åˆ¤æ–­: {decision}")
    return decision

def rag(state: GraphState) -> dict:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã€é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ã™ã‚‹"""
    print("  [Node] RAGæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
    q = state["query"]
    hits, kw = agent_search_and_pick(VECDB, q)
    return {"hits": hits, "keywords": kw}

def summarize(state: GraphState) -> dict:
    """å–å¾—ã—ãŸæƒ…å ±ã¾ãŸã¯è³ªå•ã‚’å…ƒã«ã€æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹"""
    q = state["query"]
    hits     = state.get("hits")
    keywords = state.get("keywords")
    
    if hits:
        print("  [Node] æ¤œç´¢çµæœã‚’å…ƒã«å›ç­”ã‚’ç”Ÿæˆä¸­...")
        answer = make_summary(hits, q, keywords)
    else:
        print("  [Node] æ¤œç´¢çµæœãŒãªã„ãŸã‚ã€ç›´æ¥å›ç­”ã‚’ç”Ÿæˆä¸­...")
        answer = LLM.invoke(f"200å­—ä»¥å†…ã§ã€åŒ»å¸«ã¨ã—ã¦å°‚é–€çš„ã‹ã¤æ–­å®šçš„ã«ç­”ãˆã¦ä¸‹ã•ã„: {q}").content
        
    return {"answer": answer}

# â”€â”€ Graph -------------------------------------------------
# ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã¨ã—ã¦ã€ä¸Šã§å®šç¾©ã—ãŸGraphStateã‚¯ãƒ©ã‚¹ã‚’æŒ‡å®š
graph = StateGraph(GraphState)

graph.add_node("rag",       rag)
graph.add_node("summarize", summarize)

graph.set_conditional_entry_point(
    router,
    {
        "rag": "rag",
        "direct_answer": "summarize",
    }
)

graph.add_edge("rag", "summarize")

chat_agent = graph.compile()

# â”€â”€ CLI ----------------------------------------------------
if __name__ == "__main__":
    print("AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒèµ·å‹•ã—ã¾ã—ãŸã€‚è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(çµ‚äº†ã™ã‚‹ã«ã¯ exit ã¾ãŸã¯ quit)")
    try:
        while True:
            q = input("\nğŸ—¨ï¸  > ").strip()
            if q.lower() in {"exit", "quit"}:
                print("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            if not q:
                continue
            
            # invokeã«æ¸¡ã™è¾æ›¸ã®ã‚­ãƒ¼ã‚’GraphStateã®ã‚­ãƒ¼ã¨ä¸€è‡´ã•ã›ã‚‹
            res = chat_agent.invoke({"query": q})
            
            print("\nâœ… å›ç­”\n" + "="*20)
            print(res["answer"].strip())
            print("="*20 + "\n")
            
    except (EOFError, KeyboardInterrupt):
        print("\nã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        pass